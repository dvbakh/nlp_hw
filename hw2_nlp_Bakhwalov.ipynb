{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb7169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Корпус я составил на основе данных из НКРЯ, большую часть предложений взял из подкурпуса социальных сетей.\n",
    "#Там есть сленг, например, слова \"имба\", \"ботать\", \"кринж\" и т.п;\n",
    "#Некоторые общепринятые сокращения вроде \"г.\", \"м.\", \"др.\";\n",
    "#Слова с дефисным написанием, например, \"мать-и-мачеха\",\"школа-гимназия\" и т.п.;\n",
    "#Слова, в которых для эмоциональности удвоены буквы: \"Ужжжасно\",\"Ур-ра-а\";\n",
    "#Имена собственные\n",
    "#Аббревиатуры\n",
    "\n",
    "#Для основы разметки я взял теггинг Spacy с некоторыми изменениями: предлог PREP, все союзы CONJ. \n",
    "#Он мне показался наиболее удобным для разметки, так как есть отдельные теги для имен собственных, для символов, пунктуации и т.п."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1f4c19e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Obtaining dependency information for stanza from https://files.pythonhosted.org/packages/88/4f/064015f46172c860b02148db65acd67e4925900b426f66cd0f5729d1c0d1/stanza-1.6.1-py3-none-any.whl.metadata\n",
      "  Downloading stanza-1.6.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting emoji (from stanza)\n",
      "  Obtaining dependency information for emoji from https://files.pythonhosted.org/packages/96/c6/0114b2040a96561fd1b44c75df749bbd3c898bf8047fb5ce8d7590d2dee6/emoji-2.8.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading emoji-2.8.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\dimba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (1.23.5)\n",
      "Collecting protobuf>=3.15.0 (from stanza)\n",
      "  Obtaining dependency information for protobuf>=3.15.0 from https://files.pythonhosted.org/packages/c2/59/f89c04923d68595d359f4cd7adbbdf5e5d791257945f8873d88b2fd1f979/protobuf-4.24.4-cp310-abi3-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.24.4-cp310-abi3-win_amd64.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: requests in c:\\users\\dimba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (2.28.1)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\dimba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (2.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dimba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stanza) (4.64.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\dimba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.3.0->stanza) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dimba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.3.0->stanza) (4.4.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\dimba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.3.0->stanza) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\dimba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.3.0->stanza) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dimba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.3.0->stanza) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\dimba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->stanza) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dimba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dimba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->stanza) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dimba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->stanza) (2022.9.24)\n",
      "Requirement already satisfied: colorama in c:\\users\\dimba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->stanza) (0.4.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dimba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\dimba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
      "Downloading stanza-1.6.1-py3-none-any.whl (881 kB)\n",
      "   ---------------------------------------- 881.2/881.2 kB 3.1 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.24.4-cp310-abi3-win_amd64.whl (430 kB)\n",
      "   --------------------------------------- 430.5/430.5 kB 13.6 MB/s eta 0:00:00\n",
      "Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
      "   ---------------------------------------- 358.9/358.9 kB 7.6 MB/s eta 0:00:00\n",
      "Installing collected packages: protobuf, emoji, stanza\n",
      "Successfully installed emoji-2.8.0 protobuf-4.24.4 stanza-1.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7ae423e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 15:53:56 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7c0634c0424b2aa5a5531df4353126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 15:53:57 INFO: Loading these models for language: ru (Russian):\n",
      "================================\n",
      "| Processor | Package          |\n",
      "--------------------------------\n",
      "| tokenize  | syntagrus        |\n",
      "| pos       | syntagrus_charlm |\n",
      "================================\n",
      "\n",
      "2023-10-09 15:53:57 INFO: Using device: cpu\n",
      "2023-10-09 15:53:57 INFO: Loading: tokenize\n",
      "2023-10-09 15:53:57 INFO: Loading: pos\n",
      "2023-10-09 15:53:58 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import json\n",
    "import jsonlines\n",
    "\n",
    "stz = stanza.Pipeline(lang='ru', processors='tokenize,pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "753b1525",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"корпус.txt\", 'r', encoding='utf-8') as textt:\n",
    "    for line in textt:\n",
    "        doc = stz(line)\n",
    "        for snt in doc.sentences:\n",
    "            for word in snt.words:\n",
    "                dictty={\"word\": word.text, \"pos\": word.upos}\n",
    "                with jsonlines.open('razbor_stz.jsonl', mode='a') as razbor:\n",
    "                    razbor.write(dictty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "23fca98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#происходит соотношение моего теггинга и станза\n",
    "\n",
    "\n",
    "with jsonlines.open('razbor_stz.jsonl', mode='r') as stz:\n",
    "    stz_ar=[]\n",
    "    for line in stz:\n",
    "        if line not in stz_ar:\n",
    "            stz_ar.append(line)\n",
    "        \n",
    "\n",
    "with jsonlines.open('razbor_0.jsonl', mode='r') as f0:\n",
    "    _0_ar=[]\n",
    "    for line in f0:\n",
    "        if line not in _0_ar:\n",
    "            _0_ar.append(line)\n",
    "        \n",
    "for i in stz_ar:\n",
    "    if i['pos']=='CCONJ' or i['pos']=='SCONJ':\n",
    "        i['pos']='CONJ'\n",
    "    elif i['pos']=='AUX':\n",
    "        i['pos']='VERB'\n",
    "    elif i['pos']=='ADP':\n",
    "        i['pos']='PREP'\n",
    "    elif i['pos']=='DET':\n",
    "        i['pos']='ADJ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a895f47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "По моему корпусу точность Stanza составляет 77.51 %\n"
     ]
    }
   ],
   "source": [
    "#происходит высчитывание количества правильных определений, по отношению к эталону\n",
    "#затем определяю точность, делю количество праавильных определений на общее кол-во (кроме пунктуации)\n",
    "\n",
    "match=0\n",
    "num=len(_0_ar)\n",
    "\n",
    "\n",
    "for i in _0_ar:\n",
    "        for a in stz_ar:\n",
    "            if i['word'] == a['word']:\n",
    "                if i['pos'] == a['pos']:\n",
    "                    match+=1\n",
    "\n",
    "print('По моему корпусу точность Stanza составляет', round(match/num*100, 2), '%')\n",
    "#Это результат первого теггера, stanza "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f0b9809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Делаем то же самое с pymorphy\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "m = MorphAnalyzer()\n",
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "with open(\"корпус.txt\", 'r', encoding='utf-8') as texttt:\n",
    "    for line in texttt:\n",
    "        for token in simple_word_tokenize(line):\n",
    "            doc = m.parse(token)[0]\n",
    "            dictty={\"word\": doc.word, \"pos\": doc.tag.POS}\n",
    "            with jsonlines.open('razbor_pmr.jsonl', mode='a') as razbor:\n",
    "                razbor.write(dictty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f183c6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmr_arr=[]\n",
    "with jsonlines.open('razbor_pmr.jsonl', mode='r') as pmr:\n",
    "    for line in pmr:\n",
    "        if line not in pmr_arr:\n",
    "            pmr_arr.append(line)\n",
    "        \n",
    "for i in pmr_arr:\n",
    "    if i['pos']=='ADJS' or i['pos']=='ADJF':\n",
    "        i['pos']='ADJ'\n",
    "    elif i['pos']=='PRTS' or i['pos']=='INFN':\n",
    "        i['pos']='VERB'\n",
    "    elif i['pos']=='PRCL':\n",
    "        i['pos']='PART'\n",
    "    elif i['pos']=='ADVB':\n",
    "        i['pos']='ADV'\n",
    "    elif i['pos']=='NPRO':\n",
    "        i['pos']='PRON'\n",
    "    elif i['pos']=='NUMR':\n",
    "        i['pos']='NUM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9962ace1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "По моему корпусу точность pymorphy составляет 78.47 %\n"
     ]
    }
   ],
   "source": [
    "match_2=0\n",
    "num=len(_0_ar)\n",
    "\n",
    "\n",
    "for i in _0_ar:\n",
    "        for a in pmr_arr:\n",
    "            if i['word'] == a['word']:\n",
    "                if i['pos'] == a['pos']:\n",
    "                    match_2+=1\n",
    "\n",
    "print('По моему корпусу точность pymorphy составляет', round(match_2/num*100, 2), '%')\n",
    "#Это результат второго теггера, pymorphy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "88300b9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Теперь возьмем Наташу\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "82c9d516",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"корпус.txt\", 'r', encoding='utf-8') as texttt:\n",
    "    for line in texttt:\n",
    "        doc = Doc(line)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "        doc.tokens\n",
    "        for i in doc.tokens:\n",
    "            dictty={\"word\": i.text, \"pos\": i.pos}\n",
    "            with jsonlines.open('razbor_ntsh.jsonl', mode='a') as razbor:\n",
    "                razbor.write(dictty)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5bba493b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "По моему корпусу точность natasha составляет 70.81 %\n"
     ]
    }
   ],
   "source": [
    "ntsh_arr=[]\n",
    "with jsonlines.open('razbor_ntsh.jsonl', mode='r') as ntsh:\n",
    "    for line in ntsh:\n",
    "        if line not in ntsh_arr:\n",
    "            ntsh_arr.append(line)\n",
    "\n",
    "for i in ntsh_arr:\n",
    "    if i['pos']=='CCONJ' or i['pos']=='SCONJ':\n",
    "        i['pos']='CONJ'\n",
    "    elif i['pos']=='AUX':\n",
    "        i['pos']='VERB'\n",
    "    elif i['pos']=='ADP':\n",
    "        i['pos']='PREP'\n",
    "    elif i['pos']=='DET':\n",
    "        i['pos']='ADJ'\n",
    "        \n",
    "match_3=0\n",
    "num=len(_0_ar)\n",
    "\n",
    "\n",
    "for i in _0_ar:\n",
    "        for a in ntsh_arr:\n",
    "            if i['word'] == a['word']:\n",
    "                if i['pos'] == a['pos']:\n",
    "                    match_3+=1\n",
    "\n",
    "print('По моему корпусу точность natasha составляет', round(match_3/num*100, 2), '%')\n",
    "#Это результат третьего теггера, natasha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7dc210",
   "metadata": {},
   "source": [
    " Получилось, что лучший теггер это pymorphy\n",
    " \n",
    " Кажется, самое очевидное, что модет быть в отзыве, это конструкция типа глаг.+нар., в котором наречие чаще всего показывает отношение говорящего. \n",
    " Также, мне кажется, показательными будут наречия меры степени типа \"очень\" с наречиями или прилагательными\n",
    " У меня был сериал, поэтому еще, наверное важно смотреть на конструкции типа \"сериал\"+прил.\n",
    " Мне кажется, эти шаблоны выделят основные слова, которые отображают отношение автора текста и, соответсвенно, тональность текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c894b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#пытаюсь написать чанкер\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "m = MorphAnalyzer()\n",
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "import csv\n",
    "\n",
    "\n",
    "with open(\"корпус.txt\", 'r', encoding='utf-8') as texttt:\n",
    "    for line in texttt:\n",
    "        for token in simple_word_tokenize(line):\n",
    "            doc = m.parse(token)[0]\n",
    "            if doc.tag.POS!=None:\n",
    "                with open(\"корпус1.txt\", 'a', encoding='utf-8') as f:\n",
    "                    f.write('-'.join([doc.word, doc.tag.POS]) + ' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99f1a93",
   "metadata": {},
   "source": [
    "Возьмем, к примеру, шаблоны типа \"не + глагол\", \"частица + сущ.\" и \"предл. + прил. + сущ.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "a43161b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "не признать\n",
      "не стоят\n"
     ]
    }
   ],
   "source": [
    "with open(\"корпус1.txt\", 'r', encoding='utf-8') as te:\n",
    "    for line in te:\n",
    "        tookens=''.join(line)\n",
    "        \n",
    "tokens_sp=tookens.split()\n",
    "\n",
    "for i in range(len(tokens_sp)):\n",
    "    meta1=tokens_sp[i].split('-')\n",
    "    try:\n",
    "        meta2=tokens_sp[i+1].split('-')\n",
    "    except:\n",
    "        Exception\n",
    "        \n",
    "    if meta1[0] == 'не':\n",
    "        if meta2[1] == 'VERB' or meta2[1] == 'INFN':\n",
    "            print(meta1[0] + ' ' + meta2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d238fd0a",
   "metadata": {},
   "source": [
    "Он нашел всего две формы, но, кажется, других и нет. Все работает. Проверим на других шаблонах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "e7f2a9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "это ученица\n",
      "бы счастье\n",
      "ведь роспись\n",
      "это образ\n",
      "бы кринж\n"
     ]
    }
   ],
   "source": [
    "with open(\"корпус1.txt\", 'r', encoding='utf-8') as te:\n",
    "    for line in te:\n",
    "        tookens=''.join(line)\n",
    "        \n",
    "tokens_sp=tookens.split()\n",
    "\n",
    "for i in range(len(tokens_sp)):\n",
    "    meta1=tokens_sp[i].split('-')\n",
    "    try:\n",
    "        meta2=tokens_sp[i+1].split('-')\n",
    "    except:\n",
    "        Exception\n",
    "        \n",
    "    if meta1[1] == 'PRCL':\n",
    "        if meta2[1] == 'NOUN':\n",
    "            print(meta1[0] + ' ' + meta2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d1a09324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "на летней площадке\n",
      "от нашей кухни\n",
      "по воронежской области\n",
      "в жилом доме\n",
      "по всей площади\n",
      "о своём образовании\n",
      "у чужих ворот\n",
      "в какие рамки\n",
      "при всем неприятии\n",
      "в ботанических книгах\n",
      "на одном дыхании\n"
     ]
    }
   ],
   "source": [
    "with open(\"корпус1.txt\", 'r', encoding='utf-8') as te:\n",
    "    for line in te:\n",
    "        tookens=''.join(line)\n",
    "        \n",
    "tokens_sp=tookens.split()\n",
    "\n",
    "for i in range(len(tokens_sp)):\n",
    "    meta1=tokens_sp[i].split('-')\n",
    "    try:\n",
    "        meta2=tokens_sp[i+1].split('-')\n",
    "        meta3=tokens_sp[i+2].split('-')\n",
    "    except:\n",
    "        Exception\n",
    "    if meta1[1] == 'PREP':\n",
    "        if meta2[1] == 'ADJF' or meta1[1] == 'ADJS':\n",
    "            if meta3[1] == 'NOUN':\n",
    "                print(meta1[0] + ' ' + meta2[0] + ' ' + meta3[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bf15ef",
   "metadata": {},
   "source": [
    "Получается как-то так. Все работает. Прошлая домашка, как я понял, была сделала не очень правильно, поэтому не вижу смысла на ней применять эту штуку. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
